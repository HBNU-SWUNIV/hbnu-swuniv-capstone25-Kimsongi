import asyncio
import cv2
import mediapipe as mp
import multiprocessing as mproc
import numpy as np
from keras.models import load_model
import joblib
from collections import deque
from livekit import rtc
from PIL import ImageFont, ImageDraw, Image
import queue
import sys
import threading
from livekit_auth import create_token

# ì„¤ì •ê°’
SERVER_URL = "ws://192.168.0.72:7880"  # sfu ì„œë²„ ip ì£¼ì†Œ ëŒ€ì…
ACCESS_TOKEN = create_token("ksl_worker", "dev-room")
MODEL_PATH = "models/gesture_lstm_model_dual_v4.h5" # lstm ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ëŒ€ì…
ENCODER_PATH = "processed_lstm/label_encoder_lstm_dual.pkl" # preprocess/*.pkl íŒŒì¼ ê²½ë¡œ ëŒ€ì…
T5_MODEL_PATH = "G:/ë‚´ ë“œë¼ì´ë¸Œ/my_finetuned_t5_model"
FRAMES_PER_SEQUENCE = 30
FONT_PATH = "C:/Windows/Fonts/malgun.ttf"
CONFIDENCE_THRESHOLD = 0.75
PREDICTION_INTERVAL = 3

prediction_result = ("", 0.0)
sentence_words = []
sentence_lock = threading.Lock()
generated_sentence = ""
is_predicting = False

# ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë¡œë“œ
if mproc.current_process().name == "MainProcess":
    print("â–¶ ëª¨ë¸ê³¼ ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì¤‘...")
    try:
        model = load_model(MODEL_PATH)
        label_encoder = joblib.load(ENCODER_PATH)
        print("âœ… LSTM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)
    mp_drawing = mp.solutions.drawing_utils

    frame_buffer = deque(maxlen=FRAMES_PER_SEQUENCE)

# í•œê¸€ í…ìŠ¤íŠ¸ ì¶œë ¥ í•¨ìˆ˜
_font_cache = {}
def draw_korean_text(img, text, position, font_size=32, color=(255, 255, 255), max_width=None):
    if font_size not in _font_cache:
        try:
            _font_cache[font_size] = ImageFont.truetype(FONT_PATH, font_size)
            print(f"[DEBUG] Loaded font at size {font_size} from {FONT_PATH}")
        except OSError as e:
            print(f"[DEBUG] Font load failed ({FONT_PATH}): {e}, using default font")
            _font_cache[font_size] = ImageFont.load_default()
    font = _font_cache[font_size]

    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    x, y = position
    line_height = font_size + 5
    if max_width is None:
        max_width = img.shape[1] * 0.9 - x

    words = text.split(' ')
    current_line = []
    current_y = y

    for word in words:
        test_line = ' '.join(current_line + [word])
        bbox = font.getbbox(test_line)
        text_width = bbox[2] - bbox[0]
        if text_width < max_width:
            current_line.append(word)
        else:
            draw.text((x, current_y), ' '.join(current_line), font=font, fill=color)
            current_y += line_height
            current_line = [word]
    if current_line:
        draw.text((x, current_y), ' '.join(current_line), font=font, fill=color)

    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# T5 ë¬¸ì¥ ìƒì„± í”„ë¡œì„¸ìŠ¤ í•¨ìˆ˜
def t5_worker(input_queue, output_queue, t5_model_path):
    import torch
    from transformers import T5ForConditionalGeneration, T5TokenizerFast as T5Tokenizer
    torch.set_num_threads(1)

    try:
        tokenizer = T5Tokenizer.from_pretrained(t5_model_path, local_files_only=True)
        t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path, local_files_only=True)
        print("âœ… T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ T5 ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

    while True:
        words = input_queue.get()
        print(f"[DEBUG] T5 ì…ë ¥ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸: {words}")
        prompt = f"ë¬¸ì¥ ìƒì„±: {', '.join(words)}"
        print(f"ğŸ“ T5 ì…ë ¥: '{prompt}'")

        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = t5_model.generate(
                inputs.input_ids,
                max_length=64,
                num_beams=5,
                early_stopping=True,
                repetition_penalty=2.0,
                no_repeat_ngram_size=2,
            )
        result_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)
        output_queue.put(result_sentence)
        print(f"âœ… T5 ìƒì„± ë¬¸ì¥: {result_sentence}")

# LSTM ì œìŠ¤ì²˜ ì˜ˆì¸¡ ìŠ¤ë ˆë“œ í•¨ìˆ˜
def predict_gesture(sequence_data):
    global prediction_result, is_predicting, sentence_words, generated_sentence

    #print("[DEBUG] LSTM ì˜ˆì¸¡ ì‹œì‘ (sequence_data shape:", sequence_data.shape, ")")
    prediction = model.predict(sequence_data, verbose=0)
    confidence = np.max(prediction)
    predicted_index = np.argmax(prediction)
    predicted_label = label_encoder.inverse_transform([predicted_index])[0]
    print(f"[DEBUG] LSTM ì˜ˆì¸¡ ê²°ê³¼: label={predicted_label}, confidence={confidence:.4f}")

    prediction_result = (predicted_label, confidence)

    if confidence >= CONFIDENCE_THRESHOLD and (not sentence_words or sentence_words[-1] != predicted_label):
        if predicted_label == "OK":
            print("[DEBUG] 'OK' ì œìŠ¤ì²˜ ê°ì§€ â†’ T5 ì…ë ¥ íë¡œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì „ì†¡")
            print("[DEBUG] LSTM ì˜ˆì¸¡ ì¢…ë£Œ")
            if sentence_words:
                print(f"[DEBUG] í˜„ì¬ sentence_words: {sentence_words}")
                t5_input_queue.put(list(sentence_words))
                sentence_words.clear()
        elif predicted_label == "nothing" :
                pass
        else:
            generated_sentence = ""
            sentence_words.append(predicted_label)
            print(f"â• ë‹¨ì–´ ì¶”ê°€: {predicted_label} (í˜„ì¬ ë¦¬ìŠ¤íŠ¸: {sentence_words})")

    is_predicting = False
    

# --- LiveKit ë£¨í”„ ---
room = None
frame_queue = queue.Queue(maxsize=1)
livekit_task = None
connection_failed = threading.Event()
sentence_queue = asyncio.Queue()
livekit_loop = None

async def receive_frames(stream, source_identity):
    """ì›ê²© ë¹„ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹ """
    print(f"[DEBUG] receive_frames ì‹œì‘ (source_identity={source_identity})")
    frame_idx = 0
    async for event in stream:
        converted_frame = event.frame.convert(rtc.VideoBufferType.RGB24)
        image = np.frombuffer(converted_frame.data, dtype=np.uint8)
        image = image.reshape((converted_frame.height, converted_frame.width, 3))
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

        if frame_queue.full():
            try:
                frame_queue.get_nowait()
                print("[DEBUG] frame_queue ê°€ë“ì°¸ â†’ ê°€ì¥ ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±°")
            except queue.Empty:
                pass
        frame_queue.put_nowait(image)

        frame_idx += 1
        if frame_idx % 30 == 0:
            print(f"[DEBUG] {source_identity} ë¡œë¶€í„° {frame_idx}ê°œ í”„ë ˆì„ ìˆ˜ì‹  ì¤‘ "
                  f"({converted_frame.width}x{converted_frame.height})")

async def receive_from_livekit():
    global room
    print("[DEBUG] LiveKit Room ê°ì²´ ì´ˆê¸°í™”")
    room = rtc.Room()

    @room.on("participant_connected")
    def on_participant_connected(participant):
        print(f"[DEBUG] participant_connected: identity={participant.identity}, sid={participant.sid}")

    @room.on("participant_disconnected")
    def on_participant_disconnected(participant):
        global livekit_task
        print(f"[DEBUG] participant_disconnected: identity={participant.identity}, sid={participant.sid}")
        if participant.identity == "ksl" and livekit_task is not None:
            print("[DEBUG] ksl ì°¸ê°€ì ì—°ê²° ì¢…ë£Œ â†’ livekit_task cancel")
            livekit_task.cancel()
            livekit_task = None

    @room.on("track_published")
    def on_track_published(publication, participant):
        print(f"[DEBUG] track_published: participant={participant.identity}, "
              f"kind={publication.kind}, source={publication.source}")
        if participant.identity == "ksl" and publication.kind == rtc.TrackKind.KIND_VIDEO:
            publication.set_subscribed(True)
            print("[DEBUG] ksl ì°¸ê°€ìì˜ ë¹„ë””ì˜¤ íŠ¸ë™ publish ê°ì§€ â†’ set_subscribed(True)")

    @room.on("track_subscribed")
    def on_track_subscribed(track, publication, participant):
        global livekit_task
        print(f"[DEBUG] track_subscribed: participant={participant.identity}, "
              f"track.kind={track.kind}, publication.source={publication.source}")
        if track.kind == rtc.TrackKind.KIND_VIDEO:
            print(f"[DEBUG] {participant.identity} ë¹„ë””ì˜¤ íŠ¸ë™ ìˆ˜ì‹  ì‹œì‘")
            video_stream = rtc.VideoStream(track)
            livekit_task = asyncio.create_task(receive_frames(video_stream, participant.identity))

    @room.on("disconnected")
    def on_disconnected(reason):
        print(f"[DEBUG] LiveKit disconnected: reason={reason}")
        connection_failed.set()

    try:
        print(f"[DEBUG] LiveKit ì„œë²„ì— ì—°ê²° ì‹œë„: {SERVER_URL}")
        await room.connect(SERVER_URL, ACCESS_TOKEN, rtc.RoomOptions(auto_subscribe=False))
        print("[DEBUG] LiveKit ì„œë²„ ì—°ê²° ì„±ê³µ")
        print("[DEBUG] í˜„ì¬ remote_participants ëª©ë¡:")
        for p in room.remote_participants.values():
            print(f"    - identity={p.identity}, sid={p.sid}")
    except rtc.ConnectError as e:
        print(f"[DEBUG] LiveKit ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        connection_failed.set()
        return

    # ksl ì°¸ê°€ìì˜ ê¸°ì¡´ publishëœ íŠ¸ë™ì´ ìˆìœ¼ë©´ êµ¬ë…
    if "ksl" in room.remote_participants:
        print("[DEBUG] remote_participantsì— 'ksl' ë°œê²¬ â†’ ë¹„ë””ì˜¤ íŠ¸ë™ êµ¬ë… ì‹œë„")
        for publication in room.remote_participants["ksl"].track_publications.values():
            print(f"[DEBUG] ksl track_publication: kind={publication.kind}, source={publication.source}")
            if publication.kind == rtc.TrackKind.KIND_VIDEO:
                publication.set_subscribed(True)
                print("[DEBUG] ksl ë¹„ë””ì˜¤ íŠ¸ë™ set_subscribed(True)")
                break
    else:
        print("[DEBUG] í˜„ì¬ 'ksl' ì°¸ê°€ìê°€ ì—†ìŒ (í•¸ë“œí°/AVDì—ì„œ ì•„ì§ ë°©ì— ì•ˆ ë“¤ì–´ì˜¨ ìƒíƒœì¼ ìˆ˜ ìˆìŒ)")

    asyncio.create_task(send_message_loop(room))
    print("[DEBUG] receive_from_livekit ì™„ë£Œ, ì´ë²¤íŠ¸ ë£¨í”„ ëŒ€ê¸° ìƒíƒœë¡œ ì§„ì…")
    await asyncio.Future()  # ë¬´í•œ ëŒ€ê¸°

async def send_message_loop(room):
    print("[DEBUG] send_message_loop ì‹œì‘")
    while True:
        sentence = await sentence_queue.get()
        data = sentence.encode()
        print(f"[DEBUG] LiveKit ë°ì´í„° ì±„ë„ë¡œ ë¬¸ì¥ ì „ì†¡: '{sentence}' -> destination ['asl']")
        await room.local_participant.publish_data(data, destination_identities=["asl"])

def run_livekit_background():
    global livekit_loop
    print("[DEBUG] LiveKit ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘")
    livekit_loop = asyncio.new_event_loop()
    asyncio.set_event_loop(livekit_loop)
    try:
        livekit_loop.run_until_complete(receive_from_livekit())
    finally:
        livekit_loop.close()
        print("[DEBUG] LiveKit ë°±ê·¸ë¼ìš´ë“œ ì´ë²¤íŠ¸ ë£¨í”„ ì¢…ë£Œ")

# --- ë©”ì¸ ë£¨í”„ ---
if __name__ == "__main__":
    mproc.freeze_support()
    mproc.set_start_method("spawn", force=True)

    print("[DEBUG] T5 ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìƒì„±")
    t5_input_queue = mproc.Queue()
    t5_output_queue = mproc.Queue()
    mproc.Process(target=t5_worker, args=(t5_input_queue, t5_output_queue, T5_MODEL_PATH), daemon=True).start()

    mode = int(input("ëª¨ë“œ ì„ íƒ (1: ë¡œì»¬ ì¹´ë©”ë¼, 2: ì„œë²„ ëª¨ë‹ˆí„°ë§) - "))
    print(f"[DEBUG] ì„ íƒí•œ ëª¨ë“œ: {mode}")
    if not 1 <= mode <= 2:
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")
        sys.exit(1)

    if mode == 1:
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        print("[DEBUG] ë¡œì»¬ ì¹´ë©”ë¼ ëª¨ë“œ: ì›¹ìº  ì—´ê¸° ì„±ê³µ")
    elif mode == 2:
        print("[DEBUG] ì„œë²„ ëª¨ë‹ˆí„°ë§ ëª¨ë“œ: LiveKit ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘")
        threading.Thread(target=run_livekit_background, daemon=True).start()

    print("â–¶ ì‹¤ì‹œê°„ ìˆ˜ì–´ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤. ('q': ì¢…ë£Œ, 'ìŠ¤í˜ì´ìŠ¤ë°”': ì´ˆê¸°í™”)")
    frame_count = 0
    last_frame = np.zeros((640, 480, 3), dtype=np.uint8)

    while True:
        if mode == 1:
            if not cap.isOpened():
                print("[DEBUG] ì¹´ë©”ë¼ê°€ ë” ì´ìƒ ì—´ë ¤ ìˆì§€ ì•ŠìŒ, ë£¨í”„ ì¢…ë£Œ")
                break
            ret, frame = cap.read()
            if not ret:
                print("[DEBUG] ì¹´ë©”ë¼ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨, ë£¨í”„ ì¢…ë£Œ")
                break
        elif mode == 2:
            if connection_failed.is_set():
                print("SFU ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("[DEBUG] connection_failed í”Œë˜ê·¸ê°€ setë¨, ë£¨í”„ ì¢…ë£Œ")
                break

            try:
                frame = frame_queue.get(timeout=0.1)
                last_frame = frame
            except queue.Empty:
                frame = last_frame

        frame = cv2.flip(frame, 1)
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(rgb_frame)

        if results.multi_hand_landmarks and results.multi_handedness:
            hand_data = {"Left": [], "Right": []}
            for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
                hand_label = hand_handedness.classification[0].label
                hand_data[hand_label] = [lm for lm in hand_landmarks.landmark]

            normalized_left = [0.0] * 63
            if hand_data["Left"]:
                left_wrist = hand_data["Left"][0]
                for i, lm in enumerate(hand_data["Left"]):
                    normalized_left[i * 3:i * 3 + 3] = [
                        lm.x - left_wrist.x,
                        lm.y - left_wrist.y,
                        lm.z - left_wrist.z,
                    ]

            normalized_right = [0.0] * 63
            if hand_data["Right"]:
                right_wrist = hand_data["Right"][0]
                for i, lm in enumerate(hand_data["Right"]):
                    normalized_right[i * 3:i * 3 + 3] = [
                        lm.x - right_wrist.x,
                        lm.y - right_wrist.y,
                        lm.z - right_wrist.z,
                    ]

            one_frame = normalized_left + normalized_right
            frame_buffer.append(one_frame)
        else:
            frame_buffer.clear()

        frame_count += 1
        if len(frame_buffer) == FRAMES_PER_SEQUENCE and not is_predicting and frame_count % PREDICTION_INTERVAL == 0:
            is_predicting = True
            sequence_data = np.array(frame_buffer).reshape(1, FRAMES_PER_SEQUENCE, 126)
            threading.Thread(target=predict_gesture, args=(sequence_data,), daemon=True).start()

        try:
            current_sentence = t5_output_queue.get_nowait()
            print(f"[DEBUG] T5 ì›Œì»¤ë¡œë¶€í„° ìƒˆ ë¬¸ì¥ ìˆ˜ì‹ : {current_sentence}")
            if mode == 2 and livekit_loop is not None:
                print(f"[DEBUG] LiveKit ì „ì†¡ íì— ë¬¸ì¥ ì¶”ê°€: {current_sentence}")
                asyncio.run_coroutine_threadsafe(sentence_queue.put(current_sentence), livekit_loop)
            generated_sentence = current_sentence
        except queue.Empty:
            current_sentence = generated_sentence

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        label, conf = prediction_result
        feedback_text = f"Guess: {label} ({conf:.2f})"
        color = (0, 255, 0) if conf >= CONFIDENCE_THRESHOLD else (255, 0, 0)
        # frame = cv2.resize(frame, (480, 640))
        frame = draw_korean_text(frame, feedback_text, (10, 30), font_size=32, color=color)

        with sentence_lock:
            words_text = " ".join(sentence_words)
        frame = draw_korean_text(
            frame,
            f"ì…ë ¥: {words_text}",
            (10, 80),
            font_size=40,
            color=(255, 235, 59),
            max_width=frame.shape[1] - 20,
        )

        if current_sentence:
            frame = draw_korean_text(
                frame,
                f"ê²°ê³¼: {current_sentence}",
                (10, 130),
                font_size=40,
                color=(129, 212, 250),
                max_width=frame.shape[1] - 20,
            )

        cv2.imshow("KSL Translator", frame)
        key = cv2.waitKey(1) & 0xFF

        if key == ord("q"):
            print("[DEBUG] 'q' ì…ë ¥ â†’ ë©”ì¸ ë£¨í”„ ì¢…ë£Œ")
            break
        elif key == ord(" "):
            sentence_words.clear()
            generated_sentence = ""
            prediction_result = ("", 0.0)
            print("ğŸ”„ ë¬¸ì¥ ë° ë‹¨ì–´ ëª©ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    if mode == 1:
        cap.release()
        print("[DEBUG] ë¡œì»¬ ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ í•´ì œ ì™„ë£Œ")
    elif mode == 2 and room is not None and livekit_loop is not None:
        print("[DEBUG] LiveKit room.disconnect ìš”ì²­")
        asyncio.run_coroutine_threadsafe(room.disconnect(), livekit_loop).result()
    cv2.destroyAllWindows()
    print("[DEBUG] OpenCV ìœˆë„ìš° ë‹«ê¸° ì™„ë£Œ")